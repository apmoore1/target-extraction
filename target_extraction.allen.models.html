

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>target_extraction.allen.models package &mdash; Target Extraction 0.0.1 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> Target Extraction
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <!-- Local TOC -->
              <div class="local-toc"><ul>
<li><a class="reference internal" href="#">target_extraction.allen.models package</a><ul>
<li><a class="reference internal" href="#subpackages">Subpackages</a></li>
<li><a class="reference internal" href="#submodules">Submodules</a></li>
<li><a class="reference internal" href="#module-target_extraction.allen.models.target_tagger">target_extraction.allen.models.target_tagger module</a></li>
<li><a class="reference internal" href="#module-target_extraction.allen.models">Module contents</a></li>
</ul>
</li>
</ul>
</div>
            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Target Extraction</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>target_extraction.allen.models package</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/target_extraction.allen.models.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="target-extraction-allen-models-package">
<h1>target_extraction.allen.models package<a class="headerlink" href="#target-extraction-allen-models-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="subpackages">
<h2>Subpackages<a class="headerlink" href="#subpackages" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="target_extraction.allen.models.target_sentiment.html">target_extraction.allen.models.target_sentiment package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="target_extraction.allen.models.target_sentiment.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="target_extraction.allen.models.target_sentiment.html#module-target_extraction.allen.models.target_sentiment.atae">target_extraction.allen.models.target_sentiment.atae module</a></li>
<li class="toctree-l2"><a class="reference internal" href="target_extraction.allen.models.target_sentiment.html#module-target_extraction.allen.models.target_sentiment.in_context">target_extraction.allen.models.target_sentiment.in_context module</a></li>
<li class="toctree-l2"><a class="reference internal" href="target_extraction.allen.models.target_sentiment.html#module-target_extraction.allen.models.target_sentiment.interactive_attention_network">target_extraction.allen.models.target_sentiment.interactive_attention_network module</a></li>
<li class="toctree-l2"><a class="reference internal" href="target_extraction.allen.models.target_sentiment.html#module-target_extraction.allen.models.target_sentiment.split_contexts">target_extraction.allen.models.target_sentiment.split_contexts module</a></li>
<li class="toctree-l2"><a class="reference internal" href="target_extraction.allen.models.target_sentiment.html#module-target_extraction.allen.models.target_sentiment.util">target_extraction.allen.models.target_sentiment.util module</a></li>
<li class="toctree-l2"><a class="reference internal" href="target_extraction.allen.models.target_sentiment.html#module-target_extraction.allen.models.target_sentiment">Module contents</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-target_extraction.allen.models.target_tagger">
<span id="target-extraction-allen-models-target-tagger-module"></span><h2>target_extraction.allen.models.target_tagger module<a class="headerlink" href="#module-target_extraction.allen.models.target_tagger" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="target_extraction.allen.models.target_tagger.TargetTagger">
<em class="property">class </em><code class="sig-prename descclassname">target_extraction.allen.models.target_tagger.</code><code class="sig-name descname">TargetTagger</code><span class="sig-paren">(</span><em class="sig-param">vocab</em>, <em class="sig-param">text_field_embedder</em>, <em class="sig-param">pos_tag_embedding=None</em>, <em class="sig-param">pos_tag_loss=None</em>, <em class="sig-param">label_namespace='labels'</em>, <em class="sig-param">encoder=None</em>, <em class="sig-param">feedforward=None</em>, <em class="sig-param">label_encoding=None</em>, <em class="sig-param">crf=True</em>, <em class="sig-param">include_start_end_transitions=True</em>, <em class="sig-param">constrain_crf_decoding=None</em>, <em class="sig-param">calculate_span_f1=None</em>, <em class="sig-param">dropout=None</em>, <em class="sig-param">verbose_metrics=False</em>, <em class="sig-param">initializer=&lt;allennlp.nn.initializers.InitializerApplicator object&gt;</em>, <em class="sig-param">regularizer=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/target_extraction/allen/models/target_tagger.html#TargetTagger"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#target_extraction.allen.models.target_tagger.TargetTagger" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">allennlp.models.model.Model</span></code></p>
<p>The <code class="docutils literal notranslate"><span class="pre">TargetTagger</span></code> encodes a sequence of text with an optional 
<code class="docutils literal notranslate"><span class="pre">Seq2SeqEncoder</span></code>, then uses either Conditional Random Field 
or simply a softmax model to predict a tag for each token in the sequence.</p>
<p>This is in affect the same as the <code class="docutils literal notranslate"><span class="pre">CrfTagger</span></code> with the following 
differences:</p>
<ol class="arabic simple">
<li><p>It allows you to not have to use a <code class="docutils literal notranslate"><span class="pre">Seq2SeqEncoder</span></code></p></li>
<li><p>It allows you to not have to use a <code class="docutils literal notranslate"><span class="pre">CRF</span></code> module but rather use a 
the simpler softmax over the logits</p></li>
</ol>
<dl>
<dt>vocab<span class="classifier"><code class="docutils literal notranslate"><span class="pre">Vocabulary</span></code>, required</span></dt><dd><p>A Vocabulary, required in order to compute sizes for input/output projections.</p>
</dd>
<dt>text_field_embedder<span class="classifier"><code class="docutils literal notranslate"><span class="pre">TextFieldEmbedder</span></code>, required</span></dt><dd><p>Used to embed the tokens <code class="docutils literal notranslate"><span class="pre">TextField</span></code> we get as input to the model.</p>
</dd>
<dt>pos_tag_embedding<span class="classifier"><code class="docutils literal notranslate"><span class="pre">Embedding</span></code>, optional (default=None).</span></dt><dd><p>Used to embed the <code class="docutils literal notranslate"><span class="pre">pos_tags</span></code> <code class="docutils literal notranslate"><span class="pre">SequenceLabelField</span></code> we get as input to the model.</p>
</dd>
<dt>pos_tag_loss: <code class="docutils literal notranslate"><span class="pre">float</span></code>, optional (default=None)</dt><dd><p>Whether to predict POS tags as an auxilary loss. The float here would 
represent the amount to scale that loss in the overall loss function.
The POS tags are predicted using a CRF if the main task uses a CRF else 
like the main task it will use greedy decoding based on softmax. NOTE 
we assume always that the label encoding for POS tags are of BIO format.</p>
</dd>
<dt>encoder<span class="classifier"><code class="docutils literal notranslate"><span class="pre">Seq2SeqEncoder</span></code>, optional (default=None)</span></dt><dd><p>The encoder that we will use in between embedding tokens and predicting output tags.</p>
</dd>
<dt>label_namespace<span class="classifier"><code class="docutils literal notranslate"><span class="pre">str</span></code>, optional (default=``labels``)</span></dt><dd><p>This is needed to compute the SpanBasedF1Measure metric.
Unless you did something unusual, the default value should be what you want.</p>
</dd>
<dt>feedforward<span class="classifier"><code class="docutils literal notranslate"><span class="pre">FeedForward</span></code>, optional, (default = None).</span></dt><dd><p>An optional feedforward layer to apply after the encoder.</p>
</dd>
<dt>label_encoding<span class="classifier"><code class="docutils literal notranslate"><span class="pre">str</span></code>, optional (default=``None``)</span></dt><dd><p>Label encoding to use when calculating span f1 and constraining
the CRF at decoding time . Valid options are “BIO”, “BIOUL”, “IOB1”, “BMES”.
Required if <code class="docutils literal notranslate"><span class="pre">calculate_span_f1</span></code> or <code class="docutils literal notranslate"><span class="pre">constrain_crf_decoding</span></code> is true.</p>
</dd>
<dt>crf: <code class="docutils literal notranslate"><span class="pre">bool</span></code>, optional (default=``True``)</dt><dd><p>Whether to use a CRF, if not then it just chooses the max label over 
the softmax (greedy decoding).</p>
</dd>
<dt>include_start_end_transitions<span class="classifier"><code class="docutils literal notranslate"><span class="pre">bool</span></code>, optional (default=``True``)</span></dt><dd><p>Whether to include start and end transition parameters in the CRF.</p>
</dd>
<dt>constrain_crf_decoding<span class="classifier"><code class="docutils literal notranslate"><span class="pre">bool</span></code>, optional (default=``None``)</span></dt><dd><p>If <code class="docutils literal notranslate"><span class="pre">True</span></code>, the CRF is constrained at decoding time to
produce valid sequences of tags. If this is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then
<code class="docutils literal notranslate"><span class="pre">label_encoding</span></code> is required. If <code class="docutils literal notranslate"><span class="pre">None</span></code> and
label_encoding is specified, this is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>.
If <code class="docutils literal notranslate"><span class="pre">None</span></code> and label_encoding is not specified, it defaults
to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
</dd>
<dt>calculate_span_f1<span class="classifier"><code class="docutils literal notranslate"><span class="pre">bool</span></code>, optional (default=``None``)</span></dt><dd><p>Calculate span-level F1 metrics during training. If this is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then
<code class="docutils literal notranslate"><span class="pre">label_encoding</span></code> is required. If <code class="docutils literal notranslate"><span class="pre">None</span></code> and
label_encoding is specified, this is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>.
If <code class="docutils literal notranslate"><span class="pre">None</span></code> and label_encoding is not specified, it defaults
to <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p>
</dd>
<dt>dropout:  <code class="docutils literal notranslate"><span class="pre">float</span></code>, optional (default=``None``). Use <a href="#id1"><span class="problematic" id="id2">`</span></a>Variational Dropout </dt><dd><p>&lt;<a class="reference external" href="https://arxiv.org/abs/1512.05287">https://arxiv.org/abs/1512.05287</a>&gt;`_ for sequence and normal 
dropout for non sequences.</p>
</dd>
<dt>verbose_metrics<span class="classifier"><code class="docutils literal notranslate"><span class="pre">bool</span></code>, optional (default = False)</span></dt><dd><p>If true, metrics will be returned per label class in addition
to the overall statistics.</p>
</dd>
<dt>initializer<span class="classifier"><code class="docutils literal notranslate"><span class="pre">InitializerApplicator</span></code>, optional (default=``InitializerApplicator()``)</span></dt><dd><p>Used to initialize the model parameters.</p>
</dd>
<dt>regularizer<span class="classifier"><code class="docutils literal notranslate"><span class="pre">RegularizerApplicator</span></code>, optional (default=``None``)</span></dt><dd><p>If provided, will be used to calculate the regularization penalty during training.</p>
</dd>
</dl>
<dl class="method">
<dt id="target_extraction.allen.models.target_tagger.TargetTagger.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">tokens</em>, <em class="sig-param">pos_tags=None</em>, <em class="sig-param">tags=None</em>, <em class="sig-param">metadata=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/target_extraction/allen/models/target_tagger.html#TargetTagger.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#target_extraction.allen.models.target_tagger.TargetTagger.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl>
<dt>tokens<span class="classifier"><code class="docutils literal notranslate"><span class="pre">Dict[str,</span> <span class="pre">torch.LongTensor]</span></code>, required</span></dt><dd><p>The output of <code class="docutils literal notranslate"><span class="pre">TextField.as_array()</span></code>, which should typically be passed directly to a
<code class="docutils literal notranslate"><span class="pre">TextFieldEmbedder</span></code>. This output is a dictionary mapping keys to <code class="docutils literal notranslate"><span class="pre">TokenIndexer</span></code>
tensors.  At its most basic, using a <code class="docutils literal notranslate"><span class="pre">SingleIdTokenIndexer</span></code> this is: <code class="docutils literal notranslate"><span class="pre">{&quot;tokens&quot;:</span>
<span class="pre">Tensor(batch_size,</span> <span class="pre">num_tokens)}</span></code>. This dictionary will have the same keys as were used
for the <code class="docutils literal notranslate"><span class="pre">TokenIndexers</span></code> when you created the <code class="docutils literal notranslate"><span class="pre">TextField</span></code> representing your
sequence.  The dictionary is designed to be passed directly to a <code class="docutils literal notranslate"><span class="pre">TextFieldEmbedder</span></code>,
which knows how to combine different word representations into a single vector per
token in your input.</p>
</dd>
<dt>pos_tags<span class="classifier"><code class="docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>, optional (default = <code class="docutils literal notranslate"><span class="pre">None</span></code>)</span></dt><dd><p>A torch tensor representing the sequence of POS tags of shape
<code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_tokens)</span></code></p>
</dd>
<dt>tags<span class="classifier"><code class="docutils literal notranslate"><span class="pre">torch.LongTensor</span></code>, optional (default = <code class="docutils literal notranslate"><span class="pre">None</span></code>)</span></dt><dd><p>A torch tensor representing the sequence of integer gold class labels of shape
<code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_tokens)</span></code>.</p>
</dd>
<dt>metadata<span class="classifier"><code class="docutils literal notranslate"><span class="pre">List[Dict[str,</span> <span class="pre">Any]]</span></code>, optional, (default = None)</span></dt><dd><p>metadata containg the original words in the sentence to be tagged under a ‘words’ key
as well as the original text under a ‘text’ key.</p>
</dd>
</dl>
<p>An output dictionary consisting of:</p>
<dl>
<dt>logits<span class="classifier"><code class="docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code></span></dt><dd><p>The logits that are the output of the <code class="docutils literal notranslate"><span class="pre">tag_projection_layer</span></code></p>
</dd>
<dt>class_probabilities: <code class="docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code></dt><dd><p>A tensor of shape <code class="docutils literal notranslate"><span class="pre">(batch_size,</span> <span class="pre">num_tokens,</span> <span class="pre">tag_vocab_size)</span></code> 
representing a distribution of the tag classes per word. NOTE 
that when using the CRF the highest class probability does not 
mean that will be the tag as that might not be globally optimal 
for the sentence.</p>
</dd>
<dt>mask<span class="classifier"><code class="docutils literal notranslate"><span class="pre">torch.LongTensor</span></code></span></dt><dd><p>The text field mask for the input tokens</p>
</dd>
<dt>tags<span class="classifier"><code class="docutils literal notranslate"><span class="pre">List[List[int]]</span></code></span></dt><dd><p>The predicted tags using the Viterbi algorithm if CRF is being used 
else they are from the max over the logits using the softmax 
approach.</p>
</dd>
<dt>loss<span class="classifier"><code class="docutils literal notranslate"><span class="pre">torch.FloatTensor</span></code>, optional</span></dt><dd><p>A scalar loss to be optimised. Only computed if gold label <code class="docutils literal notranslate"><span class="pre">tags</span></code> are provided.</p>
</dd>
<dt>words<span class="classifier"><code class="docutils literal notranslate"><span class="pre">List[str]</span></code>, optional</span></dt><dd><p>A list of tokens that were the original input into the model</p>
</dd>
<dt>text<span class="classifier"><code class="docutils literal notranslate"><span class="pre">str</span></code>, optional</span></dt><dd><p>A string that was the original text that the tokens have come from.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="target_extraction.allen.models.target_tagger.TargetTagger.get_metrics">
<code class="sig-name descname">get_metrics</code><span class="sig-paren">(</span><em class="sig-param">reset=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/target_extraction/allen/models/target_tagger.html#TargetTagger.get_metrics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#target_extraction.allen.models.target_tagger.TargetTagger.get_metrics" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a dictionary of metrics. This method will be called by
<cite>allennlp.training.Trainer</cite> in order to compute and use model metrics for early
stopping and model serialization.  We return an empty dictionary here rather than raising
as it is not required to implement metrics for a new model.  A boolean <cite>reset</cite> parameter is
passed, as frequently a metric accumulator will have some state which should be reset
between epochs. This is also compatible with [<cite>Metric`s](../training/metrics/metric.md). Metrics
should be populated during the call to `forward</cite>, with the <cite>Metric</cite> handling the accumulation of
the metric until this method is called.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">float</span></code>]</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="target_extraction.allen.models.target_tagger.TargetTagger.get_softmax_labels">
<code class="sig-name descname">get_softmax_labels</code><span class="sig-paren">(</span><em class="sig-param">class_probabilities</em>, <em class="sig-param">mask</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/target_extraction/allen/models/target_tagger.html#TargetTagger.get_softmax_labels"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#target_extraction.allen.models.target_tagger.TargetTagger.get_softmax_labels" title="Permalink to this definition">¶</a></dt>
<dd><p>This method has copied a large chunck of code from the 
<a class="reference external" href="https://github.com/allenai/allennlp/blob/master/allennlp/models/simple_tagger.py">SimpleTagger.decode</a> 
method.</p>
<dl class="simple">
<dt>class_probabilities<span class="classifier">A tensor containing the softmax scores for the </span></dt><dd><p>tags.</p>
</dd>
</dl>
<p>mask: A Tensor of 1’s and 0’s indicating whether a word exists.</p>
<p>A List of Lists where each inner list contains integers representing 
the most likely tag label index based on the softmax scores. Only
returns the tag label indexs for words that exist based on the mask
provided.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">List</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">int</span></code>]]</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="target_extraction.allen.models.target_tagger.TargetTagger.make_output_human_readable">
<code class="sig-name descname">make_output_human_readable</code><span class="sig-paren">(</span><em class="sig-param">output_dict</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/target_extraction/allen/models/target_tagger.html#TargetTagger.make_output_human_readable"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#target_extraction.allen.models.target_tagger.TargetTagger.make_output_human_readable" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts the tag ids to the actual tags.
<code class="docutils literal notranslate"><span class="pre">output_dict[&quot;tags&quot;]</span></code> is a list of lists of tag_ids,
so we use an ugly nested list comprehension.</p>
<dl class="field-list simple">
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Dict</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">str</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-target_extraction.allen.models">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-target_extraction.allen.models" title="Permalink to this headline">¶</a></h2>
</div>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Andrew Moore

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>