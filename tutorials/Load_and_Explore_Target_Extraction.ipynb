{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Load and Explore Target Extraction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apmoore1/target-extraction/blob/master/tutorials/Load_and_Explore_Target_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1jKAOJcq40t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!pip install git+git://github.com/apmoore1/target-extraction.git@master#egg=target-extraction\n",
        "!pip install altair"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rC3dzvwArEDd",
        "colab_type": "code",
        "outputId": "53d1582a-9f73-43cd-e77e-8bef644c3688",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from collections import defaultdict\n",
        "from typing import Callable, Dict, List, Optional\n",
        "\n",
        "from target_extraction.data_types import TargetTextCollection\n",
        "from target_extraction.dataset_parsers import semeval_2014, semeval_2016\n",
        "from target_extraction.dataset_parsers import wang_2017_election_twitter_train\n",
        "from target_extraction.dataset_parsers import wang_2017_election_twitter_test\n",
        "import pandas as pd\n",
        "import altair as alt\n",
        "\n",
        "alt.renderers.enable('html')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RendererRegistry.enable('html')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UIzh7HtrMTW",
        "colab_type": "text"
      },
      "source": [
        "# Loads and explore datasets\n",
        "In this notebook we will show you how to load three of the popular SemEval datasets as well as the election Twitter dataset of [Wang et al. 2017](https://www.aclweb.org/anthology/E17-1046) into the TargetTextCollection object format. Next we will show you the advantage of this easy to use format through serveral built in functions such as:\n",
        "\n",
        "1. Dataset Statistics -- counts of sentences and targets.\n",
        "2. Built in tokenization and POS tagging.\n",
        "3. Import and export to JSON format which is useful if you want to use the data in an [AllenNLP model](https://allennlp.org/)\n",
        "4. Setting the task of Target Extraction into a Sequence Labelling task\n",
        "\n",
        "## Loading datasets\n",
        "First we are going to show you how to load the following popular datasets:\n",
        "1. [SemEval 2014 task 4 Laptop domain (laptop)](http://alt.qcri.org/semeval2014/task4/). Of which the training data can be found [here](http://metashare.ilsp.gr:8080/repository/browse/semeval-2014-absa-train-data-v20-annotation-guidelines/683b709298b811e3a0e2842b2b6a04d7c7a19307f18a4940beef6a6143f937f0/) and the test data [here](http://metashare.ilsp.gr:8080/repository/browse/semeval-2014-absa-test-data-gold-annotations/b98d11cec18211e38229842b2b6a04d77591d40acd7542b7af823a54fb03a155/).\n",
        "2. [SemEval 2014 task 4 Restaurant domain (restaurant_14)](http://alt.qcri.org/semeval2014/task4/). Of which the training and the test data can be found at the same place as the laptop dataset.\n",
        "3. [SemEval 2016 task 5 Restaurant domain (restaurant_16)](http://alt.qcri.org/semeval2016/task5/). Of which the training data can be found [here](http://metashare.ilsp.gr:8080/repository/browse/semeval-2016-absa-restaurant-reviews-english-train-data-subtask-1/cd28e738562f11e59e2c842b2b6a04d703f9dae461bb4816a5d4320019407d23/) and the test data [here](http://metashare.ilsp.gr:8080/repository/browse/semeval-2016-absa-restaurant-reviews-english-test-data-gold-subtask-1/42bd97c6d17511e59dbe842b2b6a04d721d1933085814d9daed8fbcbe54c0615/).\n",
        "4. The election Twiiter dataset of [Wang et al. 2017](https://www.aclweb.org/anthology/E17-1046) (Election Twitter) can be automatically downloaded, so no need to worry about downloading this dataset manually.\n",
        "\n",
        "Assuming you have downloaded these files we will now upload them to this notebook, this will require 6 upload requests which will happen when you run the next cell; 1. The training, 2. test data for the laptop domain, 3. the training data, 4. test data for the restaurant_14, 5. the training data, and 6. test data for the restaurant_16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8ENkPsdBQv0",
        "colab_type": "code",
        "outputId": "1186d151-7553-447e-a336-8bacd6dfede0",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 461
        }
      },
      "source": [
        "from pathlib import Path\n",
        "from google.colab import files\n",
        "uploaded = {}\n",
        "for i in range(6):\n",
        "  temp_uploaded = files.upload()\n",
        "  uploaded = {**uploaded, **temp_uploaded}\n",
        "semeval_fps = [Path(key).resolve() for key in uploaded.keys()]\n",
        "semeval_fps = {semeval_path.name: semeval_path for semeval_path in semeval_fps}\n",
        "del uploaded\n",
        "# Paths to the files uploaded\n",
        "for file_name, fp in semeval_fps.items():\n",
        "  print(f'SemEval file name: {file_name}. File Path {fp}')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2301e4af-d1f9-4107-95cb-f7d41033e767\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-2301e4af-d1f9-4107-95cb-f7d41033e767\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Restaurants_Train_v2.xml to Restaurants_Train_v2.xml\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1fb3d724-399c-4f9b-81dd-3569c178b5e9\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-1fb3d724-399c-4f9b-81dd-3569c178b5e9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Laptop_Train_v2.xml to Laptop_Train_v2.xml\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d6465b79-efc8-463b-ac18-4aa2e079004b\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-d6465b79-efc8-463b-ac18-4aa2e079004b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Restaurants_Test_Gold.xml to Restaurants_Test_Gold.xml\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5a1e694a-c031-4d73-869f-ac9d69fe4d47\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-5a1e694a-c031-4d73-869f-ac9d69fe4d47\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Laptops_Test_Gold.xml to Laptops_Test_Gold.xml\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0b0dfe6e-ca76-46f0-a511-38dee8acbd3c\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-0b0dfe6e-ca76-46f0-a511-38dee8acbd3c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving ABSA16_Restaurants_Train_SB1_v2.xml to ABSA16_Restaurants_Train_SB1_v2.xml\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c54478ec-14cc-494f-b688-bc5c1d6132af\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-c54478ec-14cc-494f-b688-bc5c1d6132af\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving EN_REST_SB1_TEST.xml.gold to EN_REST_SB1_TEST.xml.gold\n",
            "SemEval file name: Restaurants_Train_v2.xml. File Path /content/Restaurants_Train_v2.xml\n",
            "SemEval file name: Laptop_Train_v2.xml. File Path /content/Laptop_Train_v2.xml\n",
            "SemEval file name: Restaurants_Test_Gold.xml. File Path /content/Restaurants_Test_Gold.xml\n",
            "SemEval file name: Laptops_Test_Gold.xml. File Path /content/Laptops_Test_Gold.xml\n",
            "SemEval file name: ABSA16_Restaurants_Train_SB1_v2.xml. File Path /content/ABSA16_Restaurants_Train_SB1_v2.xml\n",
            "SemEval file name: EN_REST_SB1_TEST.xml.gold. File Path /content/EN_REST_SB1_TEST.xml.gold\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFkzDLebEYEQ",
        "colab_type": "text"
      },
      "source": [
        "We will now load the data into TargetTextCollection format:\n",
        "\n",
        "NOTE: The `conflict` argument states whether or not to include targets or categories that contain the `conflict` sentiment. In this case we **do** want the `conflict` sentiment targets or categories thus the value is `True`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqS-VXMFoQc6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "laptop_train = semeval_2014(semeval_fps['Laptop_Train_v2.xml'], conflict=True)\n",
        "laptop_test = semeval_2014(semeval_fps['Laptops_Test_Gold.xml'], conflict=True)\n",
        "rest_14_train = semeval_2014(semeval_fps['Restaurants_Train_v2.xml'], conflict=True)\n",
        "rest_14_test = semeval_2014(semeval_fps['Restaurants_Test_Gold.xml'], conflict=True)\n",
        "rest_16_train = semeval_2016(semeval_fps['ABSA16_Restaurants_Train_SB1_v2.xml'], conflict=True)\n",
        "rest_16_test = semeval_2016(semeval_fps['EN_REST_SB1_TEST.xml.gold'], conflict=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmcfoSfB9BpD",
        "colab_type": "text"
      },
      "source": [
        "As I stated earlier the Election Twitter dataset is automatically downloaded and put within the following `cache_dir` `$HOME/.bella_tdsa/Wang 2017 Election Twitter`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xf8rNGty05xa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "election_train = wang_2017_election_twitter_train()\n",
        "election_test = wang_2017_election_twitter_test()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEgmjuB55E1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def target_lengths(dataset: TargetTextCollection, \n",
        "                   tokenizer: Callable[[str], List[str]],\n",
        "                   cumulative_percentage: bool = False) -> Dict[int, int]:\n",
        "  '''\n",
        "  :param dataset: The dataset\n",
        "  :param tokenizer: The tokenizer to use to split the target into tokens\n",
        "  :param cumulative_percentage: If the return should not be frequency counts of \n",
        "                                the number of targets but the cumulative \n",
        "                                percentage of targets with that number of tokens.\n",
        "  :returns: The dictionary where keys are the target length based on the number \n",
        "            of tokens in the target and the values are the number of targets \n",
        "            in the dataset that contain that number of tokens (same target can \n",
        "            be counted more than once if it exists in the dataset more then \n",
        "            once).\n",
        "  '''\n",
        "  lengths = defaultdict(lambda: 0)\n",
        "  for target, count in dataset.target_count().items():\n",
        "    length = len(tokenizer(target))\n",
        "    lengths[length] += count\n",
        "  if cumulative_percentage:\n",
        "    total_target_count = sum(lengths.values())\n",
        "    lengths = sorted(lengths.items(), key=lambda x: x[0])\n",
        "    temp_lengths = {}\n",
        "    current_percentage = 0.0\n",
        "    for length, count in lengths:\n",
        "      percentage = (count/total_target_count) * 100\n",
        "      temp_lengths[length] = current_percentage + percentage\n",
        "      current_percentage += percentage\n",
        "    lengths = temp_lengths\n",
        "  return lengths\n",
        "\n",
        "def target_length_dist_chart(target_length_frame: pd.DataFrame, title: str,\n",
        "                             target_limit: Optional[int] = None\n",
        "                             ) -> alt.vegalite.v4.api.Chart:\n",
        "  '''\n",
        "  :param error_type_frame: A DataFrame containing the following columns: \n",
        "                           1. Error Type, 2. Number of Samples (%),\n",
        "                           3. Number of Samples, 4. Dataset\n",
        "  :param title: A title to give to the generated plot\n",
        "  :param no_labels: Whether the Y-axis should have a label displayed\n",
        "  :returns: A plot that displays for each of the datasets the percentage of the \n",
        "            error types in that dataset as stacked coloured coloumns.\n",
        "  '''\n",
        "  # Plot error type data\n",
        "  font_size = 14\n",
        "  y_axis_color = alt.Color('Dataset')\n",
        "  y_axis = alt.Axis(title=None, labels=True, ticks=False)\n",
        "  y_col = alt.Y('Percentage',\n",
        "                scale=alt.Scale(domain=[60, 100]))\n",
        "  \n",
        "  largest_target_length = int(target_length_frame['Target Length'].max())\n",
        "  if target_limit is not None:\n",
        "    largest_target_length = target_limit\n",
        "  x_col = alt.X('Target Length', scale=alt.Scale(domain=[1, largest_target_length]))\n",
        "  target_length_frame = target_length_frame[target_length_frame['Target Length'] \n",
        "                                            <= largest_target_length]\n",
        "        \n",
        "  tooltip=['Percentage', 'Target Length', 'Dataset']\n",
        "  target_length_bar_data = {'x': x_col, 'y': y_col, \n",
        "                            'color': y_axis_color, 'tooltip': tooltip}\n",
        "  \n",
        "  chart= alt.Chart(target_length_frame).mark_line(point=True)\\\n",
        "                                       .encode(**target_length_bar_data)\n",
        "  chart.title = title\n",
        "  return chart\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw6k6CP3k7h6",
        "colab_type": "text"
      },
      "source": [
        "## Dataset Statistics\n",
        "\n",
        "Now that we have loaded these datasets we can perform some dataset statistics on them, such as the number of sentences, number of sentences that contain at least one target in it, and the number of  targets in the dataset, finally the number of unique targets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNh1tGuGYZOq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "0251c138-cbf2-4110-eb74-0a948a93e142"
      },
      "source": [
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "\n",
        "name_dataset = [('Laptop Train', laptop_train), ('Laptop Test', laptop_test),\n",
        "                ('Restaurant 14 Train', rest_14_train), \n",
        "                ('Restaurant 14 Test', rest_14_test),\n",
        "                ('Restaurant 16 Train', rest_16_train), \n",
        "                ('Restaurant 16 Test', rest_16_test),\n",
        "                ('Election Twitter Train', election_train),\n",
        "                ('ELection Twitter Test', election_test)]\n",
        "dataset_stats = defaultdict(lambda: {})\n",
        "\n",
        "for name, dataset in name_dataset:\n",
        "  num_sentences = len(dataset)\n",
        "  num_targ_sents = len(dataset.samples_with_targets())\n",
        "  num_targs = dataset.number_targets(incl_none_targets=False)\n",
        "  num_unique_targets = len(dataset.target_count(lower=True))\n",
        "  \n",
        "  dataset_stats[name]['Number Sentences'] = num_sentences\n",
        "  dataset_stats[name]['Number Target Sentences'] = num_targ_sents\n",
        "  dataset_stats[name]['Number Targets'] = num_targs\n",
        "  dataset_stats[name]['Number Uniq Targets'] = num_unique_targets\n",
        "pd.DataFrame(dataset_stats).T  "
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Number Sentences</th>\n",
              "      <th>Number Target Sentences</th>\n",
              "      <th>Number Targets</th>\n",
              "      <th>Number Uniq Targets</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Laptop Train</th>\n",
              "      <td>3045</td>\n",
              "      <td>1488</td>\n",
              "      <td>2358</td>\n",
              "      <td>955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Laptop Test</th>\n",
              "      <td>800</td>\n",
              "      <td>422</td>\n",
              "      <td>654</td>\n",
              "      <td>393</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Restaurant 14 Train</th>\n",
              "      <td>3041</td>\n",
              "      <td>2021</td>\n",
              "      <td>3693</td>\n",
              "      <td>1212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Restaurant 14 Test</th>\n",
              "      <td>800</td>\n",
              "      <td>606</td>\n",
              "      <td>1134</td>\n",
              "      <td>522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Restaurant 16 Train</th>\n",
              "      <td>2000</td>\n",
              "      <td>1708</td>\n",
              "      <td>1883</td>\n",
              "      <td>675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Restaurant 16 Test</th>\n",
              "      <td>676</td>\n",
              "      <td>587</td>\n",
              "      <td>651</td>\n",
              "      <td>289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Election Twitter Train</th>\n",
              "      <td>3210</td>\n",
              "      <td>3182</td>\n",
              "      <td>9358</td>\n",
              "      <td>1849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ELection Twitter Test</th>\n",
              "      <td>867</td>\n",
              "      <td>863</td>\n",
              "      <td>2541</td>\n",
              "      <td>751</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        Number Sentences  ...  Number Uniq Targets\n",
              "Laptop Train                        3045  ...                  955\n",
              "Laptop Test                          800  ...                  393\n",
              "Restaurant 14 Train                 3041  ...                 1212\n",
              "Restaurant 14 Test                   800  ...                  522\n",
              "Restaurant 16 Train                 2000  ...                  675\n",
              "Restaurant 16 Test                   676  ...                  289\n",
              "Election Twitter Train              3210  ...                 1849\n",
              "ELection Twitter Test                867  ...                  751\n",
              "\n",
              "[8 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8npYR_a5utG",
        "colab_type": "text"
      },
      "source": [
        "These statistics match those of the original SemEval task 4 subtask 1 2014 Laptop/Restaurant dataset [paper](https://www.aclweb.org/anthology/S14-2004), and original SemEval task 5 subtask 1 2016 Restaurant dataset [paper](https://www.aclweb.org/anthology/S16-1002). The statistics for Twitter Election do not match those of the original [paper](https://www.aclweb.org/anthology/E17-1046) but do match those from [Moore et al, 2018 paper](https://www.aclweb.org/anthology/C18-1097) that replicated the experiments and within that paper explains why it does not match the original.\n",
        "\n",
        "However the Restaurant 2016 dataset is reporting the number of (targets, categories) which means that for the target extraction task this is not ideal as we can have duplicate targets with the same spans in the same sentence as each one of those duplicate targets can have a different category. An example of this is shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDJNhXNl47Xw",
        "colab_type": "code",
        "outputId": "1a9ec885-a370-482c-ce44-9fdcf0cbbdf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "sample = rest_16_train['1086415:2']\n",
        "sample"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TargetText({'text': '$6 and there is much tasty food, all of it fresh and continually refilled.', 'text_id': '1086415:2', 'targets': ['food', 'food', 'food'], 'spans': [Span(start=27, end=31), Span(start=27, end=31), Span(start=27, end=31)], 'target_sentiments': ['positive', 'positive', 'positive'], 'categories': ['FOOD#STYLE_OPTIONS', 'FOOD#QUALITY', 'FOOD#PRICES'], 'category_sentiments': None})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVXx77-N7WCR",
        "colab_type": "text"
      },
      "source": [
        "As we can see in the example above the target food with the eaxct same span is used three times as each time it has a different category associated to it. To convert the dataset into one that only allows one target with one span use the `one_sample_per_span` function to create a new TargetTextCollection dataset (NOTE: This is only required for the Restaurant 2016 dataset as the previous datasets did not link the target and categories together):\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ut_BRAFvUYeX",
        "colab_type": "code",
        "outputId": "b9467710-b1de-4be8-d4ff-e1165e1a36fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "rest_target_16_train = rest_16_train.one_sample_per_span(remove_empty=True)\n",
        "rest_target_16_test = rest_16_test.one_sample_per_span(remove_empty=True)\n",
        "\n",
        "sample = rest_target_16_train['1086415:2']\n",
        "sample"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TargetText({'text': '$6 and there is much tasty food, all of it fresh and continually refilled.', 'text_id': '1086415:2', 'targets': ['food'], 'spans': [Span(start=27, end=31)], 'target_sentiments': None, 'categories': None, 'category_sentiments': None})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0A8cFUUUYmn",
        "colab_type": "text"
      },
      "source": [
        "As we can see above the TargetText instance now only has `food` once but we have had to remove all of the category information and sentiment. However this is perfect if we want to just perform the Target Extraction task. \n",
        "\n",
        "NOTE: The `remove_empty=True` argument is required as some of the (target, category) pairs contain targets that are None as in the target does not exist but the category does therefore setting this argument to `True` removes these None targets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wc8hpoCzAE_c",
        "colab_type": "code",
        "outputId": "e27a0cb8-8804-41ab-de39-8ed25119ccb3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "name_dataset = [('Laptop Train', laptop_train), ('Laptop Test', laptop_test),\n",
        "                ('Restaurant 14 Train', rest_14_train), \n",
        "                ('Restaurant 14 Test', rest_14_test),\n",
        "                ('Restaurant 16 Train', rest_target_16_train), \n",
        "                ('Restaurant 16 Test', rest_target_16_test),\n",
        "                ('Election Twitter Train', election_train),\n",
        "                ('Election Twitter Test', election_test)] \n",
        "dataset_stats = defaultdict(lambda: {})\n",
        "\n",
        "for name, dataset in name_dataset:\n",
        "  num_sentences = len(dataset)\n",
        "  num_targ_sents = len(dataset.samples_with_targets())\n",
        "  num_targs = dataset.number_targets()\n",
        "  \n",
        "  dataset_stats[name]['Number Sentences'] = num_sentences\n",
        "  dataset_stats[name]['Number Target Sentences'] = num_targ_sents\n",
        "  dataset_stats[name]['Number Targets'] = num_targs\n",
        "pd.DataFrame(dataset_stats).T  "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Number Sentences</th>\n",
              "      <th>Number Target Sentences</th>\n",
              "      <th>Number Targets</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Laptop Train</th>\n",
              "      <td>3045</td>\n",
              "      <td>1488</td>\n",
              "      <td>2358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Laptop Test</th>\n",
              "      <td>800</td>\n",
              "      <td>422</td>\n",
              "      <td>654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Restaurant 14 Train</th>\n",
              "      <td>3041</td>\n",
              "      <td>2021</td>\n",
              "      <td>3693</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Restaurant 14 Test</th>\n",
              "      <td>800</td>\n",
              "      <td>606</td>\n",
              "      <td>1134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Restaurant 16 Train</th>\n",
              "      <td>2000</td>\n",
              "      <td>1235</td>\n",
              "      <td>1745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Restaurant 16 Test</th>\n",
              "      <td>676</td>\n",
              "      <td>421</td>\n",
              "      <td>613</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Election Twitter Train</th>\n",
              "      <td>3210</td>\n",
              "      <td>3182</td>\n",
              "      <td>9358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Election Twitter Test</th>\n",
              "      <td>867</td>\n",
              "      <td>863</td>\n",
              "      <td>2541</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        Number Sentences  ...  Number Targets\n",
              "Laptop Train                        3045  ...            2358\n",
              "Laptop Test                          800  ...             654\n",
              "Restaurant 14 Train                 3041  ...            3693\n",
              "Restaurant 14 Test                   800  ...            1134\n",
              "Restaurant 16 Train                 2000  ...            1745\n",
              "Restaurant 16 Test                   676  ...             613\n",
              "Election Twitter Train              3210  ...            9358\n",
              "Election Twitter Test                867  ...            2541\n",
              "\n",
              "[8 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIRfq7ep-2b8",
        "colab_type": "text"
      },
      "source": [
        "Now that we have re-run the statistics of the dataset we can see that the Restaurant dataset has changed with regards to the Number of targets and target sentences. These statistics are now more in line with previous work ([link](https://www.aclweb.org/anthology/D17-1310) [link](https://www.aclweb.org/anthology/N19-1242)). However there are some reasons why they are not identical only for the Restaurant 2016 dataset, these are artifacts in the original annotation that we found which we share below:\n",
        "\n",
        "### First annotation mistake\n",
        "This is the case where the annotation stats NULL for the target but give a Span value:\n",
        "\n",
        "``` xml\n",
        "<sentence id=\"en_SchoonerOrLater_477965850:10\">\n",
        "  <text>In short, Schooner or Later couldn't have cared less about our being at their establishment.</text>\n",
        "  <Opinions>\n",
        "    <Opinion target=\"NULL\" category=\"SERVICE#GENERAL\" polarity=\"negative\" from=\"10\" to=\"27\"/>\n",
        "  </Opinions>\n",
        "</sentence>\n",
        "```\n",
        "In these cases we change target from `NULL` to `Schooner or Later` and more generally to the text that the span refers to.\n",
        "\n",
        "These mistakes add more targets to the dataset of which in the Restaurant train and test dataset there were 3 (1 of which linked to an exisitng target as in that target now existed twice with difference categories) and 1 mistakes respectively.\n",
        "\n",
        "### Second annotation mistake\n",
        "Annotation stats NULL for the target but the Span value is given but makes no sense:\n",
        "\n",
        "``` xml\n",
        "<sentence id=\"en_MercedesRestaurant_478010600:1\">\n",
        "  <text>– !</text>\n",
        "  <Opinions>\n",
        "    <Opinion target=\"service\" category=\"SERVICE#GENERAL\" polarity=\"positive\" from=\"39\" to=\"46\"/>\n",
        "    <Opinion target=\"NULL\" category=\"FOOD#QUALITY\" polarity=\"positive\" from=\"2\" to=\"21\"/>\n",
        "  </Opinions>\n",
        "</sentence>\n",
        "```\n",
        "As we can see that there is a target which is `NULL` but the spans refers to the following text `rcedes restaurant i` which just makes no sense. Therefore in these cases we leave the target as it does not exist and change the spans to `from = 0` and `to = 0`. In these case they do not affect the overall statistics it just highlights mistakes that have been made. In the Restaurant train and test dataset there were 1 and 4 mistakes respectively.\n",
        "\n",
        "### Furthermore...\n",
        "We found that in the SemEval 2014 Laptop domain dataset there is actually a sample that is in both Train and Test datasets:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKC-L-JJJKo9",
        "colab_type": "code",
        "outputId": "8ec16a38-8e03-42de-86ea-a9f8d724b044",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "laptop_train_ids = set(list(laptop_train.keys()))\n",
        "laptop_test_ids = set(list(laptop_test.keys()))\n",
        "id_in_both_train_test = laptop_test_ids.intersection(laptop_train_ids)\n",
        "id_in_both_train_test = list(id_in_both_train_test)[0]\n",
        "print(f'The ID that is in both Laptop train and test {id_in_both_train_test}')\n",
        "print(f'As shown the Sample in the Train:\\n{laptop_train[id_in_both_train_test]}')\n",
        "print(f'As shown the Sample in the Test:\\n{laptop_test[id_in_both_train_test]}')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The ID that is in both Laptop train and test 227\n",
            "As shown the Sample in the Train:\n",
            "TargetText({'text': 'If you want more information on macs I suggest going to apple.com and heading towards the macbook page for more information on the applications.', 'text_id': '227', 'targets': ['applications'], 'spans': [Span(start=131, end=143)], 'target_sentiments': ['neutral'], 'categories': None, 'category_sentiments': None})\n",
            "As shown the Sample in the Test:\n",
            "TargetText({'text': 'If you want more information on macs I suggest going to apple.com and heading towards the macbook page for more information on the applications.', 'text_id': '227', 'targets': ['applications'], 'spans': [Span(start=131, end=143)], 'target_sentiments': ['neutral'], 'categories': None, 'category_sentiments': None})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYJQZ_QhfQ6j",
        "colab_type": "text"
      },
      "source": [
        "### Lastly we are going to look at the distribution of sentiment labels\n",
        "It has been shown in prior work that the datasets are in-balanced with respect to the sentiment labels. Here we are going to show this for each of the datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kK9i8gDfjLm",
        "colab_type": "code",
        "outputId": "006f10b8-6001-4ce9-a0c3-0d1f07ce0b63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "def count_sentiments(dataset: TargetTextCollection) -> Dict[str, int]:\n",
        "  sentiment_count = Counter()\n",
        "  for target in dataset.samples_with_targets().values():\n",
        "    sentiment_count.update(target['target_sentiments'])\n",
        "  return sentiment_count\n",
        "\n",
        "in_balanced_stats = defaultdict(lambda: {})\n",
        "\n",
        "for name, dataset in name_dataset:\n",
        "  # Have to skip the Restaurant 16 dataset as we had to remove the sentiment \n",
        "  # labels to ensure that each target only occured once\n",
        "  if '16' in name:\n",
        "    continue\n",
        "  sentiment_counts = count_sentiments(dataset)\n",
        "  total_count = sum(sentiment_counts.values())\n",
        "  percentages = {sentiment: round((count/total_count)*100,2) \n",
        "                 for sentiment, count in sentiment_counts.items()}\n",
        "  \n",
        "  in_balanced_stats[name]['Number Positive (%)'] = f\"{sentiment_counts['positive']} ({percentages['positive']})\"\n",
        "  in_balanced_stats[name]['Number Neutral (%)'] = f\"{sentiment_counts['neutral']} ({percentages['neutral']})\"\n",
        "  in_balanced_stats[name]['Number Negative (%)'] = f\"{sentiment_counts['negative']} ({percentages['negative']})\"\n",
        "  if 'conflict' in percentages:\n",
        "    in_balanced_stats[name]['Number Conflict (%)'] = f\"{sentiment_counts['conflict']} ({percentages['conflict']})\"\n",
        "  else:\n",
        "    in_balanced_stats[name]['Number Conflict (%)'] = f\"0 (0)\"\n",
        "pd.DataFrame(in_balanced_stats).T  "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Number Positive (%)</th>\n",
              "      <th>Number Neutral (%)</th>\n",
              "      <th>Number Negative (%)</th>\n",
              "      <th>Number Conflict (%)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Laptop Train</th>\n",
              "      <td>987 (41.86)</td>\n",
              "      <td>460 (19.51)</td>\n",
              "      <td>866 (36.73)</td>\n",
              "      <td>45 (1.91)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Laptop Test</th>\n",
              "      <td>341 (52.14)</td>\n",
              "      <td>169 (25.84)</td>\n",
              "      <td>128 (19.57)</td>\n",
              "      <td>16 (2.45)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Restaurant 14 Train</th>\n",
              "      <td>2164 (58.6)</td>\n",
              "      <td>633 (17.14)</td>\n",
              "      <td>805 (21.8)</td>\n",
              "      <td>91 (2.46)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Restaurant 14 Test</th>\n",
              "      <td>728 (64.2)</td>\n",
              "      <td>196 (17.28)</td>\n",
              "      <td>196 (17.28)</td>\n",
              "      <td>14 (1.23)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Election Twitter Train</th>\n",
              "      <td>1366 (14.6)</td>\n",
              "      <td>3615 (38.63)</td>\n",
              "      <td>4377 (46.77)</td>\n",
              "      <td>0 (0)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Election Twitter Test</th>\n",
              "      <td>378 (14.88)</td>\n",
              "      <td>957 (37.66)</td>\n",
              "      <td>1206 (47.46)</td>\n",
              "      <td>0 (0)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                       Number Positive (%)  ... Number Conflict (%)\n",
              "Laptop Train                   987 (41.86)  ...           45 (1.91)\n",
              "Laptop Test                    341 (52.14)  ...           16 (2.45)\n",
              "Restaurant 14 Train            2164 (58.6)  ...           91 (2.46)\n",
              "Restaurant 14 Test              728 (64.2)  ...           14 (1.23)\n",
              "Election Twitter Train         1366 (14.6)  ...               0 (0)\n",
              "Election Twitter Test          378 (14.88)  ...               0 (0)\n",
              "\n",
              "[6 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p54sYntmJK0D",
        "colab_type": "text"
      },
      "source": [
        "As we can see from above non of the dataset are balanced with respect to sentiment and the Restaurant dataset would appear to be the worse where the positive class represents over 50% of the samples. However the conflict label is very under represented and therefore is usual ignored/removed when performing sentiment analysis in TDSA.\n",
        "\n",
        "## Tokenization\n",
        "\n",
        "The `target_extraction` has a few tokenization options avaliable:\n",
        "1. [Spacy](https://spacy.io/) -- Rule based tokenizer. \n",
        "2. [Stanford](https://stanfordnlp.github.io/stanfordnlp/), related [paper](https://www.aclweb.org/anthology/K18-2016) -- Neural Network based tokenizer.\n",
        "3. [Twokenize](https://github.com/brendano/ark-tweet-nlp) [related paper](https://www.cs.cmu.edu/~ark/TweetNLP/gimpel+etal.acl11.pdf) -- Twitter based tokenizer.\n",
        "4. Whitespace.\n",
        "\n",
        "Both the Spacy and Stanford tokenizers support multiple languages.\n",
        "\n",
        "To use any of the tokenizers we have to call the tokenizers method to get a related tokenizer as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6uFrcF0F3WD",
        "colab_type": "code",
        "outputId": "6b5b8cdf-474b-4683-e8e0-ffff9c0f83a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "from target_extraction import tokenizers\n",
        "\n",
        "example_sentence = \"I've had a great day today. The tokenizer's work well.\"\n",
        "\n",
        "# Get different spacy tokenizers\n",
        "# English is the default language\n",
        "spacy_tok_en = tokenizers.spacy_tokenizer()\n",
        "# German\n",
        "spacy_tok_de = tokenizers.spacy_tokenizer(lang='de')\n",
        "\n",
        "\n",
        "# Get different Stanford tokenizers\n",
        "# English again is the default language and the default treebank is EWT\n",
        "stanford_tok_en = tokenizers.stanford()\n",
        "# Dutch but with a treebank that is not default\n",
        "stanford_tok_nl = tokenizers.stanford(lang='nl', treebank='lassysmall')\n",
        "\n",
        "twitter_tok = tokenizers.ark_twokenize()\n",
        "\n",
        "# Whitespace\n",
        "whitespace_tok = tokenizers.whitespace()\n",
        "\n",
        "print(f'Spacy: {spacy_tok_en(example_sentence)}')\n",
        "print(f'Stanford: {stanford_tok_en(example_sentence)}')\n",
        "print(f'Twokenize {twitter_tok(example_sentence)}')\n",
        "print(f'Whitespace {whitespace_tok(example_sentence)}')\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Would you like to download the models for: en_ewt now? (Y/n)\n",
            "\n",
            "Default download directory: /root/stanfordnlp_resources\n",
            "Hit enter to continue or type an alternate directory.\n",
            "\n",
            "Downloading models for: en_ewt\n",
            "Download location: /root/stanfordnlp_resources/en_ewt_models.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 235M/235M [00:04<00:00, 48.5MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Download complete.  Models saved to: /root/stanfordnlp_resources/en_ewt_models.zip\n",
            "Extracting models file for: en_ewt\n",
            "Cleaning up...Done.\n",
            "Use device: cpu\n",
            "---\n",
            "Loading: tokenize\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "Done loading processors!\n",
            "---\n",
            "Would you like to download the models for: nl_lassysmall now? (Y/n)\n",
            "\n",
            "Default download directory: /root/stanfordnlp_resources\n",
            "Hit enter to continue or type an alternate directory.\n",
            "\n",
            "Downloading models for: nl_lassysmall\n",
            "Download location: /root/stanfordnlp_resources/nl_lassysmall_models.zip\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 223M/223M [00:06<00:00, 33.5MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Download complete.  Models saved to: /root/stanfordnlp_resources/nl_lassysmall_models.zip\n",
            "Extracting models file for: nl_lassysmall\n",
            "Cleaning up...Done.\n",
            "Use device: cpu\n",
            "---\n",
            "Loading: tokenize\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/nl_lassysmall_models/nl_lassysmall_tokenizer.pt', 'lang': 'nl', 'shorthand': 'nl_lassysmall', 'mode': 'predict'}\n",
            "Done loading processors!\n",
            "---\n",
            "Spacy: ['I', \"'ve\", 'had', 'a', 'great', 'day', 'today', '.', 'The', 'tokenizer', \"'s\", 'work', 'well', '.']\n",
            "Stanford: ['I', \"'ve\", 'had', 'a', 'great', 'day', 'today', '.', 'The', 'tokenizer', \"'s\", 'work', 'well', '.']\n",
            "Twokenize [\"I've\", 'had', 'a', 'great', 'day', 'today', '.', 'The', \"tokenizer's\", 'work', 'well', '.']\n",
            "Whitespace [\"I've\", 'had', 'a', 'great', 'day', 'today.', 'The', \"tokenizer's\", 'work', 'well.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "debbcG-QJAuN",
        "colab_type": "text"
      },
      "source": [
        "As we can see the Spacy and the Stanford tokenizers are more similar than the whitespacy with regards to handling full stops and hyphons.\n",
        "\n",
        "We can also use these tokenizers to tokenizse the whole of our datasets easily as shown below (in the example below we use the Spacy tokenizer):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Ch0NqWX0Q65",
        "colab_type": "code",
        "outputId": "8a6e7c24-33b2-4cac-d54a-1417a42d1025",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# Tokenize each dataset\n",
        "for name, dataset in name_dataset:\n",
        "  print(name)\n",
        "  dataset.tokenize(spacy_tok_en)\n",
        "\n",
        "sample = rest_target_16_train['1086415:2']\n",
        "print(f\"Text: {sample['text']}\\nTokens: {sample['tokenized_text']}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Laptop Train\n",
            "Laptop Test\n",
            "Restaurant 14 Train\n",
            "Restaurant 14 Test\n",
            "Restaurant 16 Train\n",
            "Restaurant 16 Test\n",
            "Election Twitter Train\n",
            "Election Twitter Test\n",
            "Text: $6 and there is much tasty food, all of it fresh and continually refilled.\n",
            "Tokens: ['$', '6', 'and', 'there', 'is', 'much', 'tasty', 'food', ',', 'all', 'of', 'it', 'fresh', 'and', 'continually', 'refilled', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYyrQDX4VAHd",
        "colab_type": "text"
      },
      "source": [
        "We see that within the TargetTextCollection each sample now has a new property (key in the internal disctionary) `tokenized_text` which contains the tokenized text for that sample.\n",
        "\n",
        "### Target Length Distribution\n",
        "Now that we know how to use the tokenizers we are going to use the Spacy tokenizer to explore the Target length distribution in the training datasets. The Target length distribution where the length of the Target is defined by the number words that make up the target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Thgmmcym_0UW",
        "colab_type": "code",
        "outputId": "4797732c-dbeb-4229-b53e-380bfcab65a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        }
      },
      "source": [
        "training_datasets = [('Laptop Train', laptop_train), ('Restaurant 14 Train', rest_14_train), \n",
        "                     ('Restaurant 16 Train', rest_target_16_train), \n",
        "                     ('Election Twitter Train', election_train)]\n",
        "dataset_names = []\n",
        "lengths = []\n",
        "percentages = []\n",
        "\n",
        "for name, dataset in training_datasets:\n",
        "  for target_length, percentage in target_lengths(dataset, spacy_tok_en, True).items():\n",
        "    dataset_names.append(name)\n",
        "    lengths.append(target_length)\n",
        "    percentages.append(percentage)\n",
        "target_length_df =  pd.DataFrame({'Target Length': lengths, 'Dataset': dataset_names, 'Percentage': percentages})\n",
        "target_length_dist_chart(target_length_df, \n",
        "                         'Target length cumulative distribution', 6)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "alt.Chart(...)"
            ],
            "text/html": [
              "\n",
              "<div id=\"altair-viz-1\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  (function(spec, embedOpt){\n",
              "    const outputDiv = document.getElementById(\"altair-viz-1\");\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.0.0?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function loadScript(lib) {\n",
              "      return new Promise(function(resolve, reject) {\n",
              "        var s = document.createElement('script');\n",
              "        s.src = paths[lib];\n",
              "        s.async = true;\n",
              "        s.onload = () => resolve(paths[lib]);\n",
              "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "      });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else if (typeof vegaEmbed === \"function\") {\n",
              "      displayChart(vegaEmbed);\n",
              "    } else {\n",
              "      loadScript(\"vega\")\n",
              "        .then(() => loadScript(\"vega-lite\"))\n",
              "        .then(() => loadScript(\"vega-embed\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-9d86a53da3bda81f36713717c2afb252\"}, \"mark\": {\"type\": \"line\", \"point\": true}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Dataset\"}, \"tooltip\": [{\"type\": \"quantitative\", \"field\": \"Percentage\"}, {\"type\": \"quantitative\", \"field\": \"Target Length\"}, {\"type\": \"nominal\", \"field\": \"Dataset\"}], \"x\": {\"type\": \"quantitative\", \"field\": \"Target Length\", \"scale\": {\"domain\": [1, 6]}}, \"y\": {\"type\": \"quantitative\", \"field\": \"Percentage\", \"scale\": {\"domain\": [60, 100]}}}, \"title\": \"Target length cumulative distribution\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.0.0.json\", \"datasets\": {\"data-9d86a53da3bda81f36713717c2afb252\": [{\"Target Length\": 1, \"Dataset\": \"Laptop Train\", \"Percentage\": 62.849872773536894}, {\"Target Length\": 2, \"Dataset\": \"Laptop Train\", \"Percentage\": 89.90670059372349}, {\"Target Length\": 3, \"Dataset\": \"Laptop Train\", \"Percentage\": 96.26802374893977}, {\"Target Length\": 4, \"Dataset\": \"Laptop Train\", \"Percentage\": 99.02459711620016}, {\"Target Length\": 5, \"Dataset\": \"Laptop Train\", \"Percentage\": 99.70313825275656}, {\"Target Length\": 6, \"Dataset\": \"Laptop Train\", \"Percentage\": 99.87277353689566}, {\"Target Length\": 1, \"Dataset\": \"Restaurant 14 Train\", \"Percentage\": 75.2233956133225}, {\"Target Length\": 2, \"Dataset\": \"Restaurant 14 Train\", \"Percentage\": 91.47034930950446}, {\"Target Length\": 3, \"Dataset\": \"Restaurant 14 Train\", \"Percentage\": 96.26320064987814}, {\"Target Length\": 4, \"Dataset\": \"Restaurant 14 Train\", \"Percentage\": 98.15867858109937}, {\"Target Length\": 5, \"Dataset\": \"Restaurant 14 Train\", \"Percentage\": 99.10641754670998}, {\"Target Length\": 6, \"Dataset\": \"Restaurant 14 Train\", \"Percentage\": 99.48551313295422}, {\"Target Length\": 1, \"Dataset\": \"Restaurant 16 Train\", \"Percentage\": 72.32091690544412}, {\"Target Length\": 2, \"Dataset\": \"Restaurant 16 Train\", \"Percentage\": 88.65329512893982}, {\"Target Length\": 3, \"Dataset\": \"Restaurant 16 Train\", \"Percentage\": 95.24355300859598}, {\"Target Length\": 4, \"Dataset\": \"Restaurant 16 Train\", \"Percentage\": 97.76504297994269}, {\"Target Length\": 5, \"Dataset\": \"Restaurant 16 Train\", \"Percentage\": 99.14040114613181}, {\"Target Length\": 6, \"Dataset\": \"Restaurant 16 Train\", \"Percentage\": 99.48424068767909}, {\"Target Length\": 1, \"Dataset\": \"Election Twitter Train\", \"Percentage\": 81.82303911092113}, {\"Target Length\": 2, \"Dataset\": \"Election Twitter Train\", \"Percentage\": 97.80936097456721}, {\"Target Length\": 3, \"Dataset\": \"Election Twitter Train\", \"Percentage\": 99.51912801880744}, {\"Target Length\": 4, \"Dataset\": \"Election Twitter Train\", \"Percentage\": 99.79696516349647}, {\"Target Length\": 5, \"Dataset\": \"Election Twitter Train\", \"Percentage\": 99.95725582389399}, {\"Target Length\": 6, \"Dataset\": \"Election Twitter Train\", \"Percentage\": 99.97862791194699}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dUrxGrlSAEk",
        "colab_type": "text"
      },
      "source": [
        "We can see that at least 95% of the targets in the training dataset have a token length between 1 and 3 in all of the datasets. Below we show what the cummulative target distribution is for all of the target lengths (interactive chart so you can zoom in and move the chart around):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dkyc61HbiA4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "7af54c78-0cf5-491c-d912-46dfad82e37f"
      },
      "source": [
        "target_length_dist_chart(target_length_df, \n",
        "                         'Target length cumulative distribution').interactive()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "alt.Chart(...)"
            ],
            "text/html": [
              "\n",
              "<div id=\"altair-viz-5\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  (function(spec, embedOpt){\n",
              "    const outputDiv = document.getElementById(\"altair-viz-5\");\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.0.0?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function loadScript(lib) {\n",
              "      return new Promise(function(resolve, reject) {\n",
              "        var s = document.createElement('script');\n",
              "        s.src = paths[lib];\n",
              "        s.async = true;\n",
              "        s.onload = () => resolve(paths[lib]);\n",
              "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "      });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else if (typeof vegaEmbed === \"function\") {\n",
              "      displayChart(vegaEmbed);\n",
              "    } else {\n",
              "      loadScript(\"vega\")\n",
              "        .then(() => loadScript(\"vega-lite\"))\n",
              "        .then(() => loadScript(\"vega-embed\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-2d1c7b5a4d99d41d0ef0e3aa4c899466\"}, \"mark\": {\"type\": \"line\", \"point\": true}, \"encoding\": {\"color\": {\"type\": \"nominal\", \"field\": \"Dataset\"}, \"tooltip\": [{\"type\": \"quantitative\", \"field\": \"Percentage\"}, {\"type\": \"quantitative\", \"field\": \"Target Length\"}, {\"type\": \"nominal\", \"field\": \"Dataset\"}], \"x\": {\"type\": \"quantitative\", \"field\": \"Target Length\", \"scale\": {\"domain\": [1, 23]}}, \"y\": {\"type\": \"quantitative\", \"field\": \"Percentage\", \"scale\": {\"domain\": [60, 100]}}}, \"selection\": {\"selector004\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"title\": \"Target length cumulative distribution\", \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.0.0.json\", \"datasets\": {\"data-2d1c7b5a4d99d41d0ef0e3aa4c899466\": [{\"Target Length\": 1, \"Dataset\": \"Laptop Train\", \"Percentage\": 62.849872773536894}, {\"Target Length\": 2, \"Dataset\": \"Laptop Train\", \"Percentage\": 89.90670059372349}, {\"Target Length\": 3, \"Dataset\": \"Laptop Train\", \"Percentage\": 96.26802374893977}, {\"Target Length\": 4, \"Dataset\": \"Laptop Train\", \"Percentage\": 99.02459711620016}, {\"Target Length\": 5, \"Dataset\": \"Laptop Train\", \"Percentage\": 99.70313825275656}, {\"Target Length\": 6, \"Dataset\": \"Laptop Train\", \"Percentage\": 99.87277353689566}, {\"Target Length\": 7, \"Dataset\": \"Laptop Train\", \"Percentage\": 99.91518235793043}, {\"Target Length\": 8, \"Dataset\": \"Laptop Train\", \"Percentage\": 99.99999999999999}, {\"Target Length\": 1, \"Dataset\": \"Restaurant 14 Train\", \"Percentage\": 75.2233956133225}, {\"Target Length\": 2, \"Dataset\": \"Restaurant 14 Train\", \"Percentage\": 91.47034930950446}, {\"Target Length\": 3, \"Dataset\": \"Restaurant 14 Train\", \"Percentage\": 96.26320064987814}, {\"Target Length\": 4, \"Dataset\": \"Restaurant 14 Train\", \"Percentage\": 98.15867858109937}, {\"Target Length\": 5, \"Dataset\": \"Restaurant 14 Train\", \"Percentage\": 99.10641754670998}, {\"Target Length\": 6, \"Dataset\": \"Restaurant 14 Train\", \"Percentage\": 99.48551313295422}, {\"Target Length\": 7, \"Dataset\": \"Restaurant 14 Train\", \"Percentage\": 99.62090441375574}, {\"Target Length\": 8, \"Dataset\": \"Restaurant 14 Train\", \"Percentage\": 99.67506092607634}, {\"Target Length\": 9, \"Dataset\": \"Restaurant 14 Train\", \"Percentage\": 99.83753046303816}, {\"Target Length\": 10, \"Dataset\": \"Restaurant 14 Train\", \"Percentage\": 99.89168697535877}, {\"Target Length\": 13, \"Dataset\": \"Restaurant 14 Train\", \"Percentage\": 99.94584348767937}, {\"Target Length\": 15, \"Dataset\": \"Restaurant 14 Train\", \"Percentage\": 99.97292174383968}, {\"Target Length\": 23, \"Dataset\": \"Restaurant 14 Train\", \"Percentage\": 99.99999999999999}, {\"Target Length\": 1, \"Dataset\": \"Restaurant 16 Train\", \"Percentage\": 72.32091690544412}, {\"Target Length\": 2, \"Dataset\": \"Restaurant 16 Train\", \"Percentage\": 88.65329512893982}, {\"Target Length\": 3, \"Dataset\": \"Restaurant 16 Train\", \"Percentage\": 95.24355300859598}, {\"Target Length\": 4, \"Dataset\": \"Restaurant 16 Train\", \"Percentage\": 97.76504297994269}, {\"Target Length\": 5, \"Dataset\": \"Restaurant 16 Train\", \"Percentage\": 99.14040114613181}, {\"Target Length\": 6, \"Dataset\": \"Restaurant 16 Train\", \"Percentage\": 99.48424068767909}, {\"Target Length\": 7, \"Dataset\": \"Restaurant 16 Train\", \"Percentage\": 99.71346704871061}, {\"Target Length\": 8, \"Dataset\": \"Restaurant 16 Train\", \"Percentage\": 99.82808022922637}, {\"Target Length\": 9, \"Dataset\": \"Restaurant 16 Train\", \"Percentage\": 99.88538681948425}, {\"Target Length\": 12, \"Dataset\": \"Restaurant 16 Train\", \"Percentage\": 99.94269340974213}, {\"Target Length\": 15, \"Dataset\": \"Restaurant 16 Train\", \"Percentage\": 100.00000000000001}, {\"Target Length\": 1, \"Dataset\": \"Election Twitter Train\", \"Percentage\": 81.82303911092113}, {\"Target Length\": 2, \"Dataset\": \"Election Twitter Train\", \"Percentage\": 97.80936097456721}, {\"Target Length\": 3, \"Dataset\": \"Election Twitter Train\", \"Percentage\": 99.51912801880744}, {\"Target Length\": 4, \"Dataset\": \"Election Twitter Train\", \"Percentage\": 99.79696516349647}, {\"Target Length\": 5, \"Dataset\": \"Election Twitter Train\", \"Percentage\": 99.95725582389399}, {\"Target Length\": 6, \"Dataset\": \"Election Twitter Train\", \"Percentage\": 99.97862791194699}, {\"Target Length\": 7, \"Dataset\": \"Election Twitter Train\", \"Percentage\": 99.99999999999999}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M11Fgl68_2qH",
        "colab_type": "text"
      },
      "source": [
        "From the chart above we can see that the 2014 Restaurant dataset has the longest target (23), Restaurant 16 (15),  Laptop (8), and the Election Twitter (7).\n",
        "\n",
        "## POS Tagging\n",
        "The `target_extraction` has a couple of POS tagging options avaliable:\n",
        "1. [Spacy](https://spacy.io/) -- Neural Network based. \n",
        "2. [Stanford](https://stanfordnlp.github.io/stanfordnlp/), related [paper](https://www.aclweb.org/anthology/K18-2016) -- Neural Network based.\n",
        "\n",
        "Both the Spacy and Stanford POS taggers support multiple languages. They also both support two different types of tag sets:\n",
        "1. [Universal POS tags (coarse)](https://universaldependencies.org/u/pos/) -- These are the same tags accross languages.\n",
        "2. Language Specific (fine) -- These are language independent tags and are usuall more fine grained than the Universal, an example of this tag set would be the [Penn Treebank](https://www.clips.uantwerpen.be/pages/mbsp-tags).\n",
        "\n",
        "**NOTE** Currently we have not got a POS tagger for the Twitter social media texts.\n",
        "\n",
        "To use any of the POS taggers we have to call the POS tagger's method to get the related POS tagger as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkzuuM8q8Sph",
        "colab_type": "code",
        "outputId": "ed6d5e59-b963-4ba1-c744-56908093c974",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        }
      },
      "source": [
        "from target_extraction import pos_taggers\n",
        "\n",
        "example_sentence = \"I've had a great day today. The tokenizer's work well.\"\n",
        "\n",
        "# Get different spacy tokenizers\n",
        "# English is the default language with the small model and here we have chosen \n",
        "# the universal tag set\n",
        "spacy_pos_en = pos_taggers.spacy_tagger(fine=False)\n",
        "# German with the small model and language independent tag set\n",
        "spacy_pos_de = pos_taggers.spacy_tagger(spacy_model_name='de_core_news_sm', \n",
        "                                        fine=True)\n",
        "\n",
        "\n",
        "# Get different Stanford tokenizers\n",
        "# English again is the default language and the default treebank is EWT, \n",
        "# using language independent tag set.\n",
        "stanford_pos_en_fine = pos_taggers.stanford(fine=True)\n",
        "# Dutch but with a treebank that is not default and language independent tag set\n",
        "stanford_pos_nl = pos_taggers.stanford(lang='nl', treebank='lassysmall', fine=True)\n",
        "# English again is the default language and the default treebank is EWT, \n",
        "# using the universal tag set\n",
        "stanford_pos_en_coarse = pos_taggers.stanford(fine=False)\n",
        "\n",
        "print(f'Stanford Language Independent: {stanford_pos_en_fine(example_sentence)}')\n",
        "print(f'Stanford Universal: {stanford_pos_en_coarse(example_sentence)}')\n",
        "print(f'Spacy: {spacy_pos_en(example_sentence)}')\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spacy models 'de_core_news_sm' not found. Downloading and installing.\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/de_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/de_core_news_sm\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "Use device: cpu\n",
            "---\n",
            "Loading: tokenize\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "---\n",
            "Loading: pos\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "Done loading processors!\n",
            "---\n",
            "Use device: cpu\n",
            "---\n",
            "Loading: tokenize\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/nl_lassysmall_models/nl_lassysmall_tokenizer.pt', 'lang': 'nl', 'shorthand': 'nl_lassysmall', 'mode': 'predict'}\n",
            "---\n",
            "Loading: pos\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/nl_lassysmall_models/nl_lassysmall_tagger.pt', 'pretrain_path': '/root/stanfordnlp_resources/nl_lassysmall_models/nl_lassysmall.pretrain.pt', 'lang': 'nl', 'shorthand': 'nl_lassysmall', 'mode': 'predict'}\n",
            "Done loading processors!\n",
            "---\n",
            "Use device: cpu\n",
            "---\n",
            "Loading: tokenize\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "---\n",
            "Loading: pos\n",
            "With settings: \n",
            "{'model_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/root/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
            "Done loading processors!\n",
            "---\n",
            "Stanford Language Independent: (['I', \"'ve\", 'had', 'a', 'great', 'day', 'today', '.', 'The', 'tokenizer', \"'s\", 'work', 'well', '.'], ['PRP', 'VBP', 'VBN', 'DT', 'JJ', 'NN', 'NN', '.', 'DT', 'NN', 'POS', 'NN', 'RB', '.'])\n",
            "Stanford Universal: (['I', \"'ve\", 'had', 'a', 'great', 'day', 'today', '.', 'The', 'tokenizer', \"'s\", 'work', 'well', '.'], ['PRON', 'AUX', 'VERB', 'DET', 'ADJ', 'NOUN', 'NOUN', 'PUNCT', 'DET', 'NOUN', 'PART', 'NOUN', 'ADV', 'PUNCT'])\n",
            "Spacy: (['I', \"'ve\", 'had', 'a', 'great', 'day', 'today', '.', 'The', 'tokenizer', \"'s\", 'work', 'well', '.'], ['PRON', 'VERB', 'VERB', 'DET', 'ADJ', 'NOUN', 'NOUN', 'PUNCT', 'DET', 'NOUN', 'PART', 'NOUN', 'ADV', 'PUNCT'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3o_2NT8cZNLv",
        "colab_type": "text"
      },
      "source": [
        "We can easily see the difference between the universal and independent tag sets.\n",
        "\n",
        "Like the tokenizers we can easily POS tag the whole dataset as shown below: \n",
        "\n",
        "**NOTE** You can call the POS taggers without having to call the tokenizers and it will POS tag and re-tokenize the text so that the tokenized tokens match the POS tags."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEy1RJKG9BMs",
        "colab_type": "code",
        "outputId": "fa3c8ccf-a810-47e2-9bd8-9aaba5acd6a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# POS tag each dataset\n",
        "for name, dataset in name_dataset:\n",
        "  print(name)\n",
        "  dataset.pos_text(spacy_pos_en)\n",
        "\n",
        "sample = rest_target_16_train['1086415:2']\n",
        "print(f\"Text: {sample['text']}\\nTokens: {sample['tokenized_text']}\\nPOS tags: {sample['pos_tags']}\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Laptop Train\n",
            "Laptop Test\n",
            "Restaurant 14 Train\n",
            "Restaurant 14 Test\n",
            "Restaurant 16 Train\n",
            "Restaurant 16 Test\n",
            "Election Twitter Train\n",
            "Election Twitter Test\n",
            "Text: $6 and there is much tasty food, all of it fresh and continually refilled.\n",
            "Tokens: ['$', '6', 'and', 'there', 'is', 'much', 'tasty', 'food', ',', 'all', 'of', 'it', 'fresh', 'and', 'continually', 'refilled', '.']\n",
            "POS tags: ['SYM', 'NUM', 'CCONJ', 'ADV', 'VERB', 'ADJ', 'ADJ', 'NOUN', 'PUNCT', 'DET', 'ADP', 'PRON', 'ADJ', 'CCONJ', 'ADV', 'VERB', 'PUNCT']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LizqSLg_dBqt",
        "colab_type": "text"
      },
      "source": [
        "## To and From JSON\n",
        "\n",
        "Once loaded into the TargetTextCollection format it is easy to export to JSON format and import into a TargetTextCollection from JSON as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzS0-jAsAPbO",
        "colab_type": "code",
        "outputId": "f0ff9a38-4ebf-4b31-ae86-eb005a4a5f45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "from target_extraction.data_types import TargetTextCollection\n",
        "\n",
        "example_json = laptop_train.to_json()[:200]\n",
        "print(f'Sample of the JSON: {example_json}')\n",
        "empty_collection = TargetTextCollection.from_json(laptop_train.to_json())\n",
        "empty_collection == laptop_train"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample of the JSON: {\"text\": \"I charge it at night and skip taking the cord with me because of the good battery life.\", \"text_id\": \"2339\", \"targets\": [\"cord\", \"battery life\"], \"spans\": [[41, 45], [74, 86]], \"target_senti\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0n2imrvC1-l",
        "colab_type": "text"
      },
      "source": [
        "As we can see above we can export to a JSON string and import from a JSON string. Below we show similar methods but instead of exporting and importing from Strings we do this from Files or more specifically File Paths:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEKt2heGC1ZN",
        "colab_type": "code",
        "outputId": "ca4b61a1-ca50-4ca4-bdb7-d79f9ee2a925",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from pathlib import Path\n",
        "import tempfile\n",
        "\n",
        "with tempfile.NamedTemporaryFile(mode='w+') as temp_file:\n",
        "  temp_fp = Path(temp_file.name)\n",
        "  laptop_train.to_json_file(temp_fp)\n",
        "  empty_collection = None\n",
        "  empty_collection = TargetTextCollection.load_json(temp_fp)\n",
        "  print(empty_collection == laptop_train)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aKxDgWzD3Fd",
        "colab_type": "text"
      },
      "source": [
        "And as expected the TargetTextCollection loaded from the JSON file is the same as the one that exported it into JSON.\n",
        "\n",
        "## Creating the sequence labelling task for Target Extraction and the True Upper limit\n",
        "The Target Extraction (TE) task within Aspect Based Sentiment Analysis (ABSA) is normally formulated as a Sequence Labelling task ([paper link](https://www.aclweb.org/anthology/P18-2094)), however unlike other sequence labelling problems that have pre-tokenized text with gold sequence labels like [NER](https://www.aclweb.org/anthology/P16-1101) TE is evaluated based on Eaxct Span Matching not Sequence Labels.\n",
        "\n",
        "This means that if we do treat the task as a sequence labelling task we can have tokenization errors and we show this below and thus show (we believe) for the first time the true upper limit of TE when treating it as a sequence labelling task due to tokenization error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTNzCGjaCgIt",
        "colab_type": "code",
        "outputId": "41c6fcc7-f691-41da-c86c-ee3176fb4678",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# Just to let you know for the Future to perform sequence_labels it requires\n",
        "# you to tokenize the TargetTextCollection first\n",
        "\n",
        "sequence_label_errors = {}\n",
        "for name, dataset in name_dataset:\n",
        "  print(name)\n",
        "  # Return errors shows if any samples have overlapping targets\n",
        "  return_errors = dataset.sequence_labels(return_errors=True)\n",
        "  sequence_label_errors[name] = return_errors\n",
        "sample = rest_target_16_train['1086415:2']\n",
        "print(sample['sequence_labels'])\n",
        "print(sample['tokenized_text'])\n",
        "print(sample['targets'])\n",
        "print(sample['spans'])"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Laptop Train\n",
            "Laptop Test\n",
            "Restaurant 14 Train\n",
            "Restaurant 14 Test\n",
            "Restaurant 16 Train\n",
            "Restaurant 16 Test\n",
            "Election Twitter Train\n",
            "Election Twitter Test\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "['$', '6', 'and', 'there', 'is', 'much', 'tasty', 'food', ',', 'all', 'of', 'it', 'fresh', 'and', 'continually', 'refilled', '.']\n",
            "['food']\n",
            "[Span(start=27, end=31)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9zAJinSuzy0",
        "colab_type": "text"
      },
      "source": [
        "### Errors when creating the sequence labelling task\n",
        "\n",
        "We next want to check if any of our datasets had any errors when creating the sequence labels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Od8Zgz4foB4I",
        "colab_type": "code",
        "outputId": "4f74fb60-c973-46aa-aedc-556504c68ccc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "for dataset_name, return_errors in sequence_label_errors.items():\n",
        "  print(f'Dataset {dataset_name}, number of errors: {len(return_errors)}')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset Laptop Train, number of errors: 0\n",
            "Dataset Laptop Test, number of errors: 0\n",
            "Dataset Restaurant 14 Train, number of errors: 0\n",
            "Dataset Restaurant 14 Test, number of errors: 0\n",
            "Dataset Restaurant 16 Train, number of errors: 0\n",
            "Dataset Restaurant 16 Test, number of errors: 0\n",
            "Dataset Election Twitter Train, number of errors: 6\n",
            "Dataset Election Twitter Test, number of errors: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6nsQInlMHem",
        "colab_type": "text"
      },
      "source": [
        "As we can see only the Election Twitter dataset has these Sequence Label Errors of which an example of one can be seen below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hpka6syMPIK",
        "colab_type": "code",
        "outputId": "cd9bdc9d-3d74-41e2-9497-e392f4c82e59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "sequence_label_errors['Election Twitter Train'][2]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TargetText({'text': '.@George_Osborne reiterates pledge to raise the personal income tax allowance to £12.5k, if the Tories win the #GE2015. #Budget2015 #Marr', 'text_id': '77050468909772800', 'targets': ['@George_Osborne', 'income tax', 'Budget2015', 'tax allowance', 'Tories'], 'spans': [Span(start=1, end=16), Span(start=57, end=67), Span(start=121, end=131), Span(start=64, end=77), Span(start=96, end=102)], 'target_sentiments': ['positive', 'positive', 'positive', 'positive', 'positive'], 'categories': None, 'category_sentiments': None, 'tokenized_text': ['.@George_Osborne', 'reiterates', 'pledge', 'to', 'raise', 'the', 'personal', 'income', 'tax', 'allowance', 'to', '£', '12.5k', ',', 'if', 'the', 'Tories', 'win', 'the', '#', 'GE2015', '.', '#', 'Budget2015', '#', 'Marr'], 'pos_tags': ['ADV', 'NOUN', 'VERB', 'PART', 'VERB', 'DET', 'ADJ', 'NOUN', 'NOUN', 'NOUN', 'ADP', 'NOUN', 'NUM', 'PUNCT', 'ADP', 'DET', 'PROPN', 'VERB', 'DET', 'SYM', 'NOUN', 'PUNCT', 'SYM', 'PROPN', 'NOUN', 'PROPN']})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVHnkmAPMegE",
        "colab_type": "text"
      },
      "source": [
        "Where in the example above there are two targets that overlap with each other: `income tax` and `tax allowance`\n",
        "\n",
        "This is a problem for sequence labelling as we would treat this as a BIO labelling task and if there is overlap as shown above we don't know if we should label `tax` as an `I` in the case of `income tax` or a `B` in the case of `tax allowance`. Therefore to avoid this problem we are going to remove these samples from our Election Train and Test datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5btSs42cNCp4",
        "colab_type": "code",
        "outputId": "f3689ed5-b24b-4cc0-b3e9-144c2a7eacf0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(len(election_train))\n",
        "election_train_errors = sequence_label_errors['Election Twitter Train']\n",
        "for error in election_train_errors:\n",
        "  del election_train[error['text_id']]\n",
        "print(len(election_train))\n",
        "\n",
        "print(len(election_test))\n",
        "election_test_errors = sequence_label_errors['Election Twitter Test']\n",
        "for error in election_test_errors:\n",
        "  del election_test[error['text_id']]\n",
        "print(len(election_test))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3210\n",
            "3204\n",
            "867\n",
            "865\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lzx6k0sJa90v",
        "colab_type": "text"
      },
      "source": [
        "### Tokenization errors example in detail\n",
        "Above we have used the Gold Spans to create the sequence labels based on the tokenization and we have printed the sequence labels, tokens, and the Gold Spans and in this case the Spans align perfectly with the tokens. However the assumption I think in the field is that tokenization does not cause any errors but this is not True as not all tokens perfectly align with the Gold Spans an example of this is shown below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ax0_gUw1fcsm",
        "colab_type": "code",
        "outputId": "83727cc0-f945-4c97-a520-b442a6dbc3a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "measures = laptop_train.exact_match_score('sequence_labels')\n",
        "recall, precision, f1, errors_analysis = measures\n",
        "print(f'Recall: {recall:.3f}\\nPrecision: {precision:.3f}\\nF1: {f1:.3f}\\n')\n",
        "\n",
        "false_positive_mistakes = errors_analysis['FP']\n",
        "false_negative_mistakes = errors_analysis['FN']\n",
        "print(f'Example of mistake return {false_positive_mistakes[0]}')\n",
        "fp_sample_mistake_id = false_positive_mistakes[0][0]\n",
        "fp_sample_mistake_span = false_positive_mistakes[0][1]\n",
        "\n",
        "fp_sample = laptop_train['2752']\n",
        "incorrect_target = fp_sample['text'][fp_sample_mistake_span.start:fp_sample_mistake_span.end]\n",
        "print(f'FP Mistake using `Golden` sequence labels:\\nText: {fp_sample[\"text\"]}'\n",
        "      f'\\nTokenised Text: {fp_sample[\"tokenized_text\"]}\\n'\n",
        "      f'Gold Sequence Labels: {fp_sample[\"sequence_labels\"]}\\n'\n",
        "      f'Gold Spans: {fp_sample[\"spans\"]}\\nGold Targets: {fp_sample[\"targets\"]}\\n'\n",
        "      f'The Span that is the mistake: {fp_sample_mistake_span}\\n'\n",
        "      f'The Target the incorrect span picked out: {incorrect_target}')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Recall: 0.994\n",
            "Precision: 0.997\n",
            "F1: 0.995\n",
            "\n",
            "Example of mistake return ('2752', Span(start=15, end=23))\n",
            "FP Mistake using `Golden` sequence labels:\n",
            "Text: I am using the external speaker- sound is good.\n",
            "Tokenised Text: ['I', 'am', 'using', 'the', 'external', 'speaker-', 'sound', 'is', 'good', '.']\n",
            "Gold Sequence Labels: ['O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'O']\n",
            "Gold Spans: [Span(start=15, end=31), Span(start=33, end=38)]\n",
            "Gold Targets: ['external speaker', 'sound']\n",
            "The Span that is the mistake: Span(start=15, end=23)\n",
            "The Target the incorrect span picked out: external\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSfWoAz9_Jit",
        "colab_type": "text"
      },
      "source": [
        "We can see why it did not pick out `external speaker` as a Multi Word Target because the tokenization did not Seperate `speaker` with the `-` thus `speaker-` was not labelled and even if it was it would be incorrect as `external speaker-` is not equal to `external speaker`\n",
        "\n",
        "### True upper limit on Target Extraction\n",
        "\n",
        "Below we show what the true F1 scores are for the different datasets with the four different tokenization methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqJLpIpxccbR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "287a775e-5488-4f6e-d41e-d14e12898848"
      },
      "source": [
        "name_tokenizer = [('Stanford', stanford_tok_en), ('Spacy', spacy_tok_en),\n",
        "                  ('Twokenize', twitter_tok), ('Whitespace', whitespace_tok)]\n",
        "dataset_tok_errors = {}\n",
        "dataset_tokenizer = defaultdict(lambda: {})\n",
        "for dataset_name, dataset in name_dataset:\n",
        "  for tokenizer_name, tokenizer in name_tokenizer:\n",
        "    try:\n",
        "      dataset.tokenize(tokenizer)\n",
        "      errors = dataset.sequence_labels(return_errors=True)\n",
        "      if errors:\n",
        "        dataset_tok_errors[f'{dataset_name} {tokenizer_name}'] = errors\n",
        "        f1 = 0.0\n",
        "      else:\n",
        "        measures = dataset.exact_match_score('sequence_labels')\n",
        "        f1 = measures[2]\n",
        "    except:\n",
        "      dataset_tok_errors[f'{dataset_name} {tokenizer_name}'] = 'Tokenization error'\n",
        "      f1 = 0.0\n",
        "    dataset_tokenizer[dataset_name][tokenizer_name] = f1\n",
        "pd.DataFrame(dataset_tokenizer).T.round(3)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Stanford</th>\n",
              "      <th>Spacy</th>\n",
              "      <th>Twokenize</th>\n",
              "      <th>Whitespace</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Laptop Train</th>\n",
              "      <td>0.997</td>\n",
              "      <td>0.995</td>\n",
              "      <td>0.994</td>\n",
              "      <td>0.749</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Laptop Test</th>\n",
              "      <td>0.992</td>\n",
              "      <td>0.993</td>\n",
              "      <td>0.991</td>\n",
              "      <td>0.715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Restaurant 14 Train</th>\n",
              "      <td>0.999</td>\n",
              "      <td>0.998</td>\n",
              "      <td>0.997</td>\n",
              "      <td>0.777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Restaurant 14 Test</th>\n",
              "      <td>1.000</td>\n",
              "      <td>0.999</td>\n",
              "      <td>0.998</td>\n",
              "      <td>0.727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Restaurant 16 Train</th>\n",
              "      <td>0.999</td>\n",
              "      <td>0.998</td>\n",
              "      <td>0.997</td>\n",
              "      <td>0.793</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Restaurant 16 Test</th>\n",
              "      <td>1.000</td>\n",
              "      <td>0.995</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.757</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Election Twitter Train</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.991</td>\n",
              "      <td>0.910</td>\n",
              "      <td>0.780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Election Twitter Test</th>\n",
              "      <td>0.962</td>\n",
              "      <td>0.991</td>\n",
              "      <td>0.915</td>\n",
              "      <td>0.785</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        Stanford  Spacy  Twokenize  Whitespace\n",
              "Laptop Train               0.997  0.995      0.994       0.749\n",
              "Laptop Test                0.992  0.993      0.991       0.715\n",
              "Restaurant 14 Train        0.999  0.998      0.997       0.777\n",
              "Restaurant 14 Test         1.000  0.999      0.998       0.727\n",
              "Restaurant 16 Train        0.999  0.998      0.997       0.793\n",
              "Restaurant 16 Test         1.000  0.995      0.000       0.757\n",
              "Election Twitter Train     0.000  0.991      0.910       0.780\n",
              "Election Twitter Test      0.962  0.991      0.915       0.785"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nsj4HjfeYClk",
        "colab_type": "code",
        "outputId": "d1b154df-dc4d-4157-8a54-d6356e5b7be6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "for dataset_tok, error in dataset_tok_errors.items():\n",
        "  dataset_name_tok_name = dataset_tok.split()\n",
        "  dataset_name = ' '.join(dataset_name_tok_name[:-1])\n",
        "  tok_name = dataset_name_tok_name[-1]\n",
        "  print(f'Error caused by {tok_name} tokenizer on the {dataset_name} dataset')\n",
        "  print(f'Error:\\n{error}')\n",
        "  print()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Error caused by Twokenize tokenizer on the Restaurant 16 Test dataset\n",
            "Error:\n",
            "Tokenization error\n",
            "\n",
            "Error caused by Stanford tokenizer on the Election Twitter Train dataset\n",
            "Error:\n",
            "[TargetText({'text': '@FearDept Oligarch Master & Emissary #MSM fear-bomb voter to tip #Israel #UK #GE2015 & #USGE2016; Works for @Iran new Oligarch on the block!', 'text_id': '72972753172578305', 'targets': ['FearDept Oligarch Master', '@FearDept', 'MSM', 'USGE2016', '@Iran', 'Israel'], 'spans': [Span(start=1, end=25), Span(start=0, end=9), Span(start=38, end=41), Span(start=88, end=96), Span(start=108, end=113), Span(start=66, end=72)], 'target_sentiments': ['negative', 'negative', 'negative', 'negative', 'negative', 'negative'], 'categories': None, 'category_sentiments': None, 'tokenized_text': ['@FearDept', 'Oligarch', 'Master', '&', 'Emissary', '#MSM', 'fear-bomb', 'voter', 'to', 'tip', '#Israel', '#UK', '#GE2015', '&', '#USGE2016;', 'Works', 'for', '@Iran', 'new', 'Oligarch', 'on', 'the', 'block!'], 'pos_tags': ['PROPN', 'PROPN', 'PROPN', 'CCONJ', 'PROPN', 'SYM', 'PROPN', 'NOUN', 'PUNCT', 'NOUN', 'NOUN', 'ADP', 'VERB', 'SYM', 'PROPN', 'SYM', 'PROPN', 'SYM', 'PROPN', 'CCONJ', 'SYM', 'PROPN', 'PUNCT', 'NOUN', 'ADP', 'NUM', 'ADJ', 'PROPN', 'ADP', 'DET', 'NOUN', 'PUNCT'], 'sequence_labels': ['B', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O']}), TargetText({'text': '@SkyNews Ask Osborne supporters how the debt reduction plan is coming along. #Budget2015 #GE2015 http://t.co/T9qO995OzT', 'text_id': '78181332699881473', 'targets': ['SkyNews Ask Osborne', '@SkyNews', 'Budget2015'], 'spans': [Span(start=1, end=20), Span(start=0, end=8), Span(start=78, end=88)], 'target_sentiments': ['negative', 'neutral', 'negative'], 'categories': None, 'category_sentiments': None, 'tokenized_text': ['@SkyNews', 'Ask', 'Osborne', 'supporters', 'how', 'the', 'debt', 'reduction', 'plan', 'is', 'coming', 'along.', '#Budget2015', '#GE2015', 'http://t.co/T9qO995OzT'], 'pos_tags': ['INTJ', 'VERB', 'PROPN', 'NOUN', 'ADV', 'DET', 'NOUN', 'NOUN', 'NOUN', 'VERB', 'VERB', 'ADV', 'PUNCT', 'SYM', 'PROPN', 'NOUN', 'NOUN', 'NOUN'], 'sequence_labels': ['B', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']})]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBVg5S0TWNX2",
        "colab_type": "text"
      },
      "source": [
        "We can see that we have two tokenization failures. \n",
        "1. The Twokenizer tokenizer failure on the Restaurant 16 test dataset was due to the tokenizer changing the input data so much so that the target span no longer aligned with the target words (very detailed error that we won't go into any further)\n",
        "2. The second is due to the target BIO overlap problem as described above in **Errors when creating the sequence labelling task**\n",
        "\n",
        "#### Results\n",
        "Above the F1 scores for the different tokenizers of which these sequences would be treated as the Gold Standard Sequences for the machine learning model, this shows very different results between an actual tokenizer and Whitespace. The difference between Stanford and Spacy is marginal but the Twitter based Twokenize tokenizer is generally worse and performs worse than Spacy on the Twitter dataset which is surprising."
      ]
    }
  ]
}